#!/usr/bin/env python
"""
å‡†å¤‡ wikitext-103 æ•°æ®é›†çš„ tokenized cache
ç”¨äº KDA å’Œ MKDA è®­ç»ƒè„šæœ¬
"""
from __future__ import annotations

import argparse
import os
from functools import partial
from typing import Any

from datasets import DatasetDict, load_dataset
from transformers import AutoTokenizer


def parse_args() -> argparse.Namespace:
    p = argparse.ArgumentParser(description="å‡†å¤‡ tokenized cache ç”¨äº wikitext-103 è®­ç»ƒ")
    
    p.add_argument(
        "--tokenizer", 
        type=str, 
        default="gpt2", 
        help="Tokenizer åç§°æˆ–æœ¬åœ°è·¯å¾„"
    )
    p.add_argument(
        "--dataset_name", 
        type=str, 
        default="wikitext", 
        help="HF datasets åç§°"
    )
    p.add_argument(
        "--dataset_config", 
        type=str, 
        default="wikitext-103-raw-v1", 
        help="HF datasets é…ç½®"
    )
    p.add_argument(
        "--text_column", 
        type=str, 
        default="text",
        help="æ–‡æœ¬åˆ—å"
    )
    p.add_argument(
        "--cache_dir", 
        type=str, 
        default=None, 
        help="HF datasets cache_dir"
    )
    p.add_argument(
        "--tokenized_cache", 
        type=str, 
        default="./data/wikitext103_gpt2_1024", 
        help="ä¿å­˜ tokenized dataset çš„è·¯å¾„"
    )
    p.add_argument(
        "--seq_len", 
        type=int, 
        default=1024,
        help="åºåˆ—é•¿åº¦"
    )
    p.add_argument(
        "--num_proc", 
        type=int, 
        default=8,
        help="å¤„ç†æ•°æ®çš„è¿›ç¨‹æ•°"
    )
    
    return p.parse_args()


def _normalize_text(example: dict[str, Any], text_column: str) -> dict[str, Any]:
    """è§„èŒƒåŒ–æ–‡æœ¬"""
    text = example.get(text_column, "")
    if text is None:
        text = ""
    return {"text": text.strip()}


def _tokenize_batch(
    examples: dict[str, list[Any]], 
    tokenizer, 
    eos_token_id: int
) -> dict[str, list[list[int]]]:
    """æ‰¹é‡tokenizeæ–‡æœ¬"""
    texts = [t for t in examples["text"] if t]
    if not texts:
        return {"input_ids": []}
    
    tokenized = tokenizer(
        texts,
        add_special_tokens=False,
        return_attention_mask=False,
        return_token_type_ids=False,
    )["input_ids"]
    tokenized = [ids + [eos_token_id] for ids in tokenized if len(ids) > 0]
    return {"input_ids": tokenized}


def _group_texts(
    examples: dict[str, list[list[int]]], 
    seq_len: int
) -> dict[str, list[list[int]]]:
    """å°†æ–‡æœ¬åˆ†ç»„ä¸ºå›ºå®šé•¿åº¦çš„åºåˆ—"""
    if not examples["input_ids"]:
        return {"input_ids": [], "labels": []}
    
    concatenated = []
    for ids in examples["input_ids"]:
        concatenated.extend(ids)
    
    total_len = (len(concatenated) // seq_len) * seq_len
    if total_len == 0:
        return {"input_ids": [], "labels": []}
    
    input_ids = [concatenated[i : i + seq_len] for i in range(0, total_len, seq_len)]
    return {"input_ids": input_ids, "labels": input_ids.copy()}


def main() -> None:
    args = parse_args()
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨
    if os.path.isdir(args.tokenized_cache):
        print(f"âš ï¸  Tokenized cache å·²å­˜åœ¨: {args.tokenized_cache}")
        print("å¦‚æœè¦é‡æ–°ç”Ÿæˆï¼Œè¯·å…ˆåˆ é™¤è¯¥ç›®å½•ã€‚")
        return
    
    print(f"ğŸ“¥ åŠ è½½ tokenizer: {args.tokenizer}")
    tokenizer = AutoTokenizer.from_pretrained(args.tokenizer, use_fast=True)
    tokenizer.model_max_length = int(1e9)
    if tokenizer.pad_token_id is None:
        tokenizer.pad_token = tokenizer.eos_token
    
    eos_id = tokenizer.eos_token_id
    if eos_id is None:
        raise ValueError("Tokenizer æ²¡æœ‰ eos_token_idï¼Œè¯·è®¾ç½®ä¸€ä¸ªå¸¦ EOS çš„ tokenizerã€‚")
    
    print(f"ğŸ“¥ åŠ è½½æ•°æ®é›†: {args.dataset_name} ({args.dataset_config})")
    print("âš ï¸  å¼ºåˆ¶é‡æ–°ä¸‹è½½ä»¥é¿å…ç¼“å­˜é—®é¢˜...")
    raw: DatasetDict = load_dataset(
        args.dataset_name,
        args.dataset_config,
        cache_dir=args.cache_dir,
        download_mode="force_redownload"
    )
    
    print("ğŸ”§ è§„èŒƒåŒ–æ–‡æœ¬...")
    raw = raw.map(
        partial(_normalize_text, text_column=args.text_column), 
        desc="è§„èŒƒåŒ–æ–‡æœ¬"
    )
    
    print("ğŸ§¹ è¿‡æ»¤ç©ºè¡Œ...")
    raw = raw.filter(lambda ex: bool(ex["text"]), desc="è¿‡æ»¤ç©ºè¡Œ")
    
    remove_cols = list(raw["train"].features.keys())
    
    print(f"ğŸ”¤ Tokenize æ–‡æœ¬ (ä½¿ç”¨ {args.num_proc} è¿›ç¨‹)...")
    tokenized = raw.map(
        partial(_tokenize_batch, tokenizer=tokenizer, eos_token_id=eos_id),
        batched=True,
        num_proc=args.num_proc,
        remove_columns=remove_cols,
        desc="Tokenize",
    )
    
    print(f"ğŸ“¦ å°†æ–‡æœ¬åˆ†ç»„ä¸º {args.seq_len} token çš„åºåˆ—...")
    lm_ds = tokenized.map(
        partial(_group_texts, seq_len=args.seq_len),
        batched=True,
        num_proc=args.num_proc,
        desc=f"åˆ†ç»„æ–‡æœ¬ (seq_len={args.seq_len})",
    )
    
    print(f"\nğŸ“Š æ•°æ®é›†ç»Ÿè®¡:")
    for split_name, split_data in lm_ds.items():
        print(f"  - {split_name}: {len(split_data)} æ ·æœ¬")
    
    print(f"\nğŸ’¾ ä¿å­˜ tokenized cache åˆ°: {args.tokenized_cache}")
    os.makedirs(args.tokenized_cache, exist_ok=True)
    lm_ds.save_to_disk(args.tokenized_cache)
    
    print(f"\nâœ… å®Œæˆï¼tokenized cache å·²ä¿å­˜åˆ°: {args.tokenized_cache}")
    print(f"\nä½¿ç”¨æ–¹æ³•:")
    print(f"  python examples/train_kda_wikitext103_epochs20.py \\")
    print(f"    --tokenized_cache {args.tokenized_cache}")


if __name__ == "__main__":
    main()
