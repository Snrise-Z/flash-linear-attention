#!/usr/bin/env python
"""æµ‹è¯• tokenized cache æ˜¯å¦å¯ä»¥æ­£ç¡®åŠ è½½"""

from datasets import load_from_disk

cache_path = "./data/wikitext103_gpt2_1024"

print(f"ğŸ“‚ åŠ è½½ tokenized cache: {cache_path}")
dataset = load_from_disk(cache_path)

print(f"\nâœ… æˆåŠŸåŠ è½½ï¼æ•°æ®é›†ä¿¡æ¯:")
print(f"  æ•°æ®é›†ç±»å‹: {type(dataset)}")
print(f"  åˆ†å‰²: {list(dataset.keys())}")

for split_name, split_data in dataset.items():
    print(f"\n  {split_name}:")
    print(f"    - æ ·æœ¬æ•°: {len(split_data)}")
    print(f"    - ç‰¹å¾: {split_data.features}")
    if len(split_data) > 0:
        example = split_data[0]
        print(f"    - ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ input_ids é•¿åº¦: {len(example['input_ids'])}")
        print(f"    - ç¬¬ä¸€ä¸ªæ ·æœ¬çš„ labels é•¿åº¦: {len(example['labels'])}")

print("\nâœ… tokenized cache éªŒè¯æˆåŠŸï¼")
